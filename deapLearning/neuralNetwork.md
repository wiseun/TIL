신경망
=====

#### 1.정의
- 퍼셉트론의 진화형.
- 신경망 또한 여러개의 뉴런이 모여서 이뤄진다.
- 퍼셉트론은 가중치를 사람이 직접 설정해 줘야 하지만 신경망은 자동으로 서정이 된다는 것이 차이점
- 또한 활성화 함수의 존재가 강조된 것도 퍼셉트론 과의 차이점.

#### 2. 활성화 함수
- 퍼셉트론에서 정의한 수식을 다시 가져와 보자.
$$
    y = 0 (w1x1 + w2x2 <= b)
        y = 1 (w1x1 + w2x2 >  b)
    $$
    - 이때 가중치를 적용한 값이 b보다 큰지 아닌지에 따라서 결과 값이 바뀌어 었다. 이러한 방식을 계단 함수(step functrion)라고 하는데 이러한 역할을 하는 함수를 활성화 함수라고 한다.
    - 그 종류로는 계단함수, sigmoid, ReLU(Rectified Linear Unit) 등이 있다.

###### 2.1 계단함수
    - 위에서 설명한 대로 기준 값이 있고 그 기준값 보다 큰지 작은지에 따라서 결정된다.
    - 그래프를 그리면 계단 형태를 띄고 있다.

###### 2.2 Sigmoid
    - 1/(1 + exp(-x))
    - Sigmoid란 s자 모양이라는 뜻이다. 그 의미대로 S자 형태를 가지고 있다.
    - 가장 중요한 것은 비선형 그래프라는 것이다. 신경망에서는 활성화 함수로 반드시 비선형 그래프를 사용해야 한다. 그래야 신경망의 층을 깊게 하는 의미가 있다.

###### 2.2 ReLU(Rectified Linear Unit)
    - 최근 유행하고 있는 함수.
    - 값이 0보다 작으면 0을 0보다 크면 그 값을 그대로 출력한다.


